{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from argparse import ArgumentParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from collections import Counter\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import logging\n",
    "import numpy as np\n",
    "import yaml\n",
    "import random\n",
    "import gc\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import cpu_count\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "config_path = '../config/20news.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_path(df_path, rand=False, rand_seed=4079):\n",
    "    df = pd.read_csv(df_path)\n",
    "    if rand:\n",
    "        df = shuffle(df, random_state=rand_seed)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(path):\n",
    "    df = load_from_path(path, rand=True)\n",
    "    df['id'] = df['id'].astype('category')\n",
    "    df['cat'] = df['cat'].astype('category')\n",
    "    df['doc'] = df['doc'].astype(str)\n",
    "    return df\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'====================Configs===================='\n",
      "'../data/20news/train.csv'\n",
      "'LABELED has 1123 data'\n",
      "'UNLABELED has 10191 data'\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "# end with\n",
    "pprint('=' * 20 + 'Configs' + '=' * 20)\n",
    "pprint(config['train'])\n",
    "\n",
    "train_df = load_df(config['train'])\n",
    "test_df = load_df(config['test'])\n",
    "\n",
    "train_df['labeled'] = 0\n",
    "#### add x% of EACH CLASS in the train_df to L\n",
    "cat_count = Counter(train_df['cat'])\n",
    "random.seed(config['seed'])\n",
    "ratio = []\n",
    "for k, v in cat_count.items():\n",
    "    ratio.append(dict(k=v / train_df.shape[0]))\n",
    "    cat_id = list(train_df[train_df['cat'] == k]['id'].values)\n",
    "    rand_id = random.sample(cat_id, int(config['percent'] * v))  # x% currently 10%\n",
    "    train_df.loc[train_df['id'].isin(rand_id), 'labeled'] = 1\n",
    "# end for\n",
    "\n",
    "l_train_df = train_df.loc[train_df['labeled'] == 1]\n",
    "u_train_df = train_df.loc[train_df['labeled'] == 0]\n",
    "pprint('LABELED has {} data'.format(l_train_df.shape[0]))\n",
    "pprint('UNLABELED has {} data'.format(u_train_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'====================Embedding with doc2vec===================='\n",
      "'DOC2VEC: Labeled training documents embedded into (1123, 300) dimensions'\n",
      "'DOC2VEC: Unlabeled training documents embedded into (10191, 300) dimensions'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embed all documents with doc2vec\n",
    "pprint('=' * 20 + 'Embedding with doc2vec' + '=' * 20)\n",
    "model = Doc2Vec.load(config['embed']['doc2vec_path'])\n",
    "l_train_doc2vec = np.array([model.infer_vector(doc.strip().split()) for doc in l_train_df['doc'].values])\n",
    "if u_train_df.shape[0] > 0:\n",
    "    u_train_doc2vec = np.array([model.infer_vector(doc.strip().split()) for doc in u_train_df['doc'].values])\n",
    "test_doc2vec = np.array([model.infer_vector(doc.strip().split()) for doc in test_df['doc'].values])\n",
    "pprint('DOC2VEC: Labeled training documents embedded into {} dimensions'.format(l_train_doc2vec.shape))\n",
    "if u_train_df.shape[0] > 0:\n",
    "    pprint('DOC2VEC: Unlabeled training documents embedded into {} dimensions'.format(u_train_doc2vec.shape))\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'====================Embedding with doc2vec===================='\n",
      "'DOC2VEC: Labeled training documents embedded into (1123, 300) dimensions'\n",
      "'DOC2VEC: Unlabeled training documents embedded into (10191, 300) dimensions'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Binarized Classes: ['alt.atheism' 'comp.graphics' 'comp.os.ms-windows.misc'\\n\"\n",
      " \" 'comp.sys.ibm.pc.hardware' 'comp.sys.mac.hardware' 'comp.windows.x'\\n\"\n",
      " \" 'misc.forsale' 'rec.autos' 'rec.motorcycles' 'rec.sport.baseball'\\n\"\n",
      " \" 'rec.sport.hockey' 'sci.crypt' 'sci.electronics' 'sci.med' 'sci.space'\\n\"\n",
      " \" 'soc.religion.christian' 'talk.politics.guns' 'talk.politics.mideast'\\n\"\n",
      " \" 'talk.politics.misc' 'talk.religion.misc']\")\n",
      "(\"Encoded Classes: ['alt.atheism' 'comp.graphics' 'comp.os.ms-windows.misc'\\n\"\n",
      " \" 'comp.sys.ibm.pc.hardware' 'comp.sys.mac.hardware' 'comp.windows.x'\\n\"\n",
      " \" 'misc.forsale' 'rec.autos' 'rec.motorcycles' 'rec.sport.baseball'\\n\"\n",
      " \" 'rec.sport.hockey' 'sci.crypt' 'sci.electronics' 'sci.med' 'sci.space'\\n\"\n",
      " \" 'soc.religion.christian' 'talk.politics.guns' 'talk.politics.mideast'\\n\"\n",
      " \" 'talk.politics.misc' 'talk.religion.misc']\")\n"
     ]
    }
   ],
   "source": [
    "#### binarize train target\n",
    "lb = LabelBinarizer().fit(train_df['cat'].values)\n",
    "l_train_cat_bin = lb.transform(l_train_df['cat'].values)\n",
    "if u_train_df.shape[0] > 0:\n",
    "    u_train_cat_bin = lb.transform(u_train_df['cat'].values)\n",
    "pprint('Binarized Classes: {}'.format(lb.classes_))\n",
    "#### binarize test target\n",
    "test_cat_bin = lb.transform(test_df['cat'].values)\n",
    "\n",
    "#### encode train target\n",
    "le = LabelEncoder().fit(train_df['cat'].values)\n",
    "l_train_cat_en = le.transform(l_train_df['cat'].values)\n",
    "if u_train_df.shape[0] > 0:\n",
    "    u_train_cat_en = le.transform(u_train_df['cat'].values)\n",
    "pprint('Encoded Classes: {}'.format(le.classes_))\n",
    "#### encode test target\n",
    "test_cat_en = le.transform(test_df['cat'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/20news/0.1per_labeled_train_featurized.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-b93c1b343acc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             id=l_train_df['id'].values[i])\n\u001b[0;32m     19\u001b[0m         for i, label in enumerate(l_train_cat_bin)]\n\u001b[1;32m---> 20\u001b[1;33m     joblib.dump(\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0ml_train_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labeled_train_out'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompress_level\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         with _write_fileobject(filename, compress=(compress_method,\n\u001b[0m\u001b[0;32m    476\u001b[0m                                                    compress_level)) as f:\n\u001b[0;32m    477\u001b[0m             \u001b[0mNumpyPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\numpy_pickle_utils.py\u001b[0m in \u001b[0;36m_write_fileobject\u001b[1;34m(filename, compress)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_buffered_write_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_instance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         file_instance = _COMPRESSORS['zlib'].compressor_file(\n\u001b[0m\u001b[0;32m    176\u001b[0m             filename, compresslevel=compresslevel)\n\u001b[0;32m    177\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_buffered_write_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_instance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\compressor.py\u001b[0m in \u001b[0;36mcompressor_file\u001b[1;34m(self, fileobj, compresslevel)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileobj_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             return self.fileobj_factory(fileobj, 'wb',\n\u001b[0m\u001b[0;32m    108\u001b[0m                                         compresslevel=compresslevel)\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\joblib\\compressor.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, compresslevel)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closefp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"write\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/20news/0.1per_labeled_train_featurized.joblib'"
     ]
    }
   ],
   "source": [
    "#### Save all embedded documents\n",
    "#### save labeled train data to output path\n",
    "if config['labeled_train_out']:\n",
    "    l_train_data = [\n",
    "        dict(\n",
    "            # fasttext=l_train_fasttext_text[i],\n",
    "            # pooledbiobert=l_train_pooledbiobert_text[i],\n",
    "            # seqbiobert=l_train_seqbiobert_text[i],\n",
    "            # tfidf=l_train_tfidf_text[i],\n",
    "            doc2vec=l_train_doc2vec[i],\n",
    "            # use=l_train_use_text[i],\n",
    "            # pooledbert=l_train_pooledbert_text[i],\n",
    "            # seqbert=l_train_seqbert_text[i],\n",
    "            # pooledelmo=l_train_pooledelmo_text[i],\n",
    "            # seqelmo=l_train_seqelmo_text[i],\n",
    "            cat_bin=label,\n",
    "            cat_en=l_train_cat_en[i],\n",
    "            id=l_train_df['id'].values[i])\n",
    "        for i, label in enumerate(l_train_cat_bin)]\n",
    "    joblib.dump(\n",
    "        l_train_data,\n",
    "        config['labeled_train_out'],\n",
    "        compress=3)\n",
    "# end if\n",
    "\n",
    "#### save unlabeled train data to output path\n",
    "if config['unlabeled_train_out'] and u_train_df.shape[0] > 0:\n",
    "    u_train_data = [\n",
    "        dict(\n",
    "            # fasttext=u_train_fasttext_text[i],\n",
    "            # pooledbiobert=u_train_pooledbiobert_text[i],\n",
    "            # seqbiobert=u_train_seqbiobert_text[i],\n",
    "            # tfidf=u_train_tfidf_text[i],\n",
    "            # doc2vec=u_train_doc2vec[i],\n",
    "            use=u_train_use_text[i],\n",
    "            # pooledbert=u_train_pooledbert_text[i],\n",
    "            # seqbert=u_train_seqbert_text[i],\n",
    "            # pooledelmo=u_train_pooledelmo_text[i],\n",
    "            # seqelmo=u_train_seqelmo_text[i],\n",
    "            cat_bin=label,\n",
    "            cat_en=u_train_cat_en[i],\n",
    "            id=u_train_df['id'].values[i])\n",
    "        for i, label in enumerate(u_train_cat_bin)]\n",
    "    joblib.dump(\n",
    "        u_train_data,\n",
    "        config['unlabeled_train_out'],\n",
    "        compress=3)\n",
    "# end if\n",
    "\n",
    "if config['test_out']:\n",
    "    test_data = [\n",
    "        dict(\n",
    "            # fasttext=test_fasttext_text[i],\n",
    "            # pooledbiobert=test_pooledbiobert_text[i],\n",
    "            # seqbiobert=test_seqbiobert_text[i],\n",
    "            # tfidf=test_tfidf_text[i],\n",
    "            doc2vec=test_doc2vec[i],\n",
    "            # use=test_use_text[i],\n",
    "            # pooledbert=test_pooledbert_text[i],\n",
    "            # seqbert=test_seqbert_text[i],\n",
    "            # pooledelmo=test_pooledelmo_text[i],\n",
    "            # seqelmo=test_seqelmo_text[i],\n",
    "            cat_bin=label,\n",
    "            cat_en=test_cat_en[i],\n",
    "            id=test_df['id'].values[i])\n",
    "        for i, label in enumerate(test_cat_bin)]\n",
    "    joblib.dump(\n",
    "        test_data,\n",
    "        config['test_out'],\n",
    "        compress=3)\n",
    "# end if\n",
    "\n",
    "#### save binarizer to output path\n",
    "if config['encoder_out']:\n",
    "    joblib.dump(\n",
    "        le,\n",
    "        config['encoder_out'],\n",
    "        compress=3)\n",
    "\n",
    "#### save encoder to output path\n",
    "if config['binarizer_out']:\n",
    "    joblib.dump(\n",
    "        lb,\n",
    "        config['binarizer_out'],\n",
    "        compress=3)\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(l_train_cat_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
