{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from argparse import ArgumentParser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from collections import Counter\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import logging\n",
    "import numpy as np\n",
    "import yaml\n",
    "import random\n",
    "import gc\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import cpu_count\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "config_path = '../config/20news.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_path(df_path, rand=False, rand_seed=4079):\n",
    "    df = pd.read_csv(df_path)\n",
    "    if rand:\n",
    "        df = shuffle(df, random_state=rand_seed)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(path):\n",
    "    df = load_from_path(path, rand=True)\n",
    "    df['id'] = df['id'].astype('category')\n",
    "    df['cat'] = df['cat'].astype('category')\n",
    "    df['doc'] = df['doc'].astype(str)\n",
    "    return df\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'====================Configs===================='\n",
      "'../data/20news/train.csv'\n",
      "'LABELED has 1123 data'\n",
      "'UNLABELED has 10191 data'\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "# end with\n",
    "pprint('=' * 20 + 'Configs' + '=' * 20)\n",
    "pprint(config['train'])\n",
    "\n",
    "train_df = load_df(config['train'])\n",
    "test_df = load_df(config['test'])\n",
    "\n",
    "train_df['labeled'] = 0\n",
    "#### add x% of EACH CLASS in the train_df to L\n",
    "cat_count = Counter(train_df['cat'])\n",
    "random.seed(config['seed'])\n",
    "ratio = []\n",
    "for k, v in cat_count.items():\n",
    "    ratio.append(dict(k=v / train_df.shape[0]))\n",
    "    cat_id = list(train_df[train_df['cat'] == k]['id'].values)\n",
    "    rand_id = random.sample(cat_id, int(config['percent'] * v))  # x% currently 10%\n",
    "    train_df.loc[train_df['id'].isin(rand_id), 'labeled'] = 1\n",
    "# end for\n",
    "\n",
    "l_train_df = train_df.loc[train_df['labeled'] == 1]\n",
    "u_train_df = train_df.loc[train_df['labeled'] == 0]\n",
    "pprint('LABELED has {} data'.format(l_train_df.shape[0]))\n",
    "pprint('UNLABELED has {} data'.format(u_train_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'====================Embedding with doc2vec===================='\n",
      "'DOC2VEC: Labeled training documents embedded into (1123, 300) dimensions'\n",
      "'DOC2VEC: Unlabeled training documents embedded into (10191, 300) dimensions'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embed all documents with doc2vec\n",
    "pprint('=' * 20 + 'Embedding with doc2vec' + '=' * 20)\n",
    "model = Doc2Vec.load(config['embed']['doc2vec_path'])\n",
    "l_train_doc2vec = np.array([model.infer_vector(doc.strip().split()) for doc in l_train_df['doc'].values])\n",
    "if u_train_df.shape[0] > 0:\n",
    "    u_train_doc2vec = np.array([model.infer_vector(doc.strip().split()) for doc in u_train_df['doc'].values])\n",
    "test_doc2vec = np.array([model.infer_vector(doc.strip().split()) for doc in test_df['doc'].values])\n",
    "pprint('DOC2VEC: Labeled training documents embedded into {} dimensions'.format(l_train_doc2vec.shape))\n",
    "if u_train_df.shape[0] > 0:\n",
    "    pprint('DOC2VEC: Unlabeled training documents embedded into {} dimensions'.format(u_train_doc2vec.shape))\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Binarized Classes: ['alt.atheism' 'comp.graphics' 'comp.os.ms-windows.misc'\\n\"\n",
      " \" 'comp.sys.ibm.pc.hardware' 'comp.sys.mac.hardware' 'comp.windows.x'\\n\"\n",
      " \" 'misc.forsale' 'rec.autos' 'rec.motorcycles' 'rec.sport.baseball'\\n\"\n",
      " \" 'rec.sport.hockey' 'sci.crypt' 'sci.electronics' 'sci.med' 'sci.space'\\n\"\n",
      " \" 'soc.religion.christian' 'talk.politics.guns' 'talk.politics.mideast'\\n\"\n",
      " \" 'talk.politics.misc' 'talk.religion.misc']\")\n",
      "(\"Encoded Classes: ['alt.atheism' 'comp.graphics' 'comp.os.ms-windows.misc'\\n\"\n",
      " \" 'comp.sys.ibm.pc.hardware' 'comp.sys.mac.hardware' 'comp.windows.x'\\n\"\n",
      " \" 'misc.forsale' 'rec.autos' 'rec.motorcycles' 'rec.sport.baseball'\\n\"\n",
      " \" 'rec.sport.hockey' 'sci.crypt' 'sci.electronics' 'sci.med' 'sci.space'\\n\"\n",
      " \" 'soc.religion.christian' 'talk.politics.guns' 'talk.politics.mideast'\\n\"\n",
      " \" 'talk.politics.misc' 'talk.religion.misc']\")\n"
     ]
    }
   ],
   "source": [
    "#### binarize train target\n",
    "lb = LabelBinarizer().fit(train_df['cat'].values)\n",
    "l_train_cat_bin = lb.transform(l_train_df['cat'].values)\n",
    "if u_train_df.shape[0] > 0:\n",
    "    u_train_cat_bin = lb.transform(u_train_df['cat'].values)\n",
    "pprint('Binarized Classes: {}'.format(lb.classes_))\n",
    "#### binarize test target\n",
    "test_cat_bin = lb.transform(test_df['cat'].values)\n",
    "\n",
    "#### encode train target\n",
    "le = LabelEncoder().fit(train_df['cat'].values)\n",
    "l_train_cat_en = le.transform(l_train_df['cat'].values)\n",
    "if u_train_df.shape[0] > 0:\n",
    "    u_train_cat_en = le.transform(u_train_df['cat'].values)\n",
    "pprint('Encoded Classes: {}'.format(le.classes_))\n",
    "#### encode test target\n",
    "test_cat_en = le.transform(test_df['cat'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save all embedded documents\n",
    "#### save labeled train data to output path\n",
    "if config['labeled_train_out']:\n",
    "    l_train_data = [\n",
    "        dict(\n",
    "            # fasttext=l_train_fasttext_text[i],\n",
    "            # pooledbiobert=l_train_pooledbiobert_text[i],\n",
    "            # seqbiobert=l_train_seqbiobert_text[i],\n",
    "            # tfidf=l_train_tfidf_text[i],\n",
    "            doc2vec=l_train_doc2vec[i],\n",
    "            # use=l_train_use_text[i],\n",
    "            # pooledbert=l_train_pooledbert_text[i],\n",
    "            # seqbert=l_train_seqbert_text[i],\n",
    "            # pooledelmo=l_train_pooledelmo_text[i],\n",
    "            # seqelmo=l_train_seqelmo_text[i],\n",
    "            cat_bin=label,\n",
    "            cat_en=l_train_cat_en[i],\n",
    "            id=l_train_df['id'].values[i])\n",
    "        for i, label in enumerate(l_train_cat_bin)]\n",
    "    joblib.dump(\n",
    "        l_train_data,\n",
    "        config['labeled_train_out'],\n",
    "        compress=3)\n",
    "# end if\n",
    "\n",
    "#### save unlabeled train data to output path\n",
    "if config['unlabeled_train_out'] and u_train_df.shape[0] > 0:\n",
    "    u_train_data = [\n",
    "        dict(\n",
    "            # fasttext=u_train_fasttext_text[i],\n",
    "            # pooledbiobert=u_train_pooledbiobert_text[i],\n",
    "            # seqbiobert=u_train_seqbiobert_text[i],\n",
    "            # tfidf=u_train_tfidf_text[i],\n",
    "            doc2vec=u_train_doc2vec[i],\n",
    "            # use=u_train_use_text[i],\n",
    "            # pooledbert=u_train_pooledbert_text[i],\n",
    "            # seqbert=u_train_seqbert_text[i],\n",
    "            # pooledelmo=u_train_pooledelmo_text[i],\n",
    "            # seqelmo=u_train_seqelmo_text[i],\n",
    "            cat_bin=label,\n",
    "            cat_en=u_train_cat_en[i],\n",
    "            id=u_train_df['id'].values[i])\n",
    "        for i, label in enumerate(u_train_cat_bin)]\n",
    "    joblib.dump(\n",
    "        u_train_data,\n",
    "        config['unlabeled_train_out'],\n",
    "        compress=3)\n",
    "# end if\n",
    "\n",
    "if config['test_out']:\n",
    "    test_data = [\n",
    "        dict(\n",
    "            # fasttext=test_fasttext_text[i],\n",
    "            # pooledbiobert=test_pooledbiobert_text[i],\n",
    "            # seqbiobert=test_seqbiobert_text[i],\n",
    "            # tfidf=test_tfidf_text[i],\n",
    "            doc2vec=test_doc2vec[i],\n",
    "            # use=test_use_text[i],\n",
    "            # pooledbert=test_pooledbert_text[i],\n",
    "            # seqbert=test_seqbert_text[i],\n",
    "            # pooledelmo=test_pooledelmo_text[i],\n",
    "            # seqelmo=test_seqelmo_text[i],\n",
    "            cat_bin=label,\n",
    "            cat_en=test_cat_en[i],\n",
    "            id=test_df['id'].values[i])\n",
    "        for i, label in enumerate(test_cat_bin)]\n",
    "    joblib.dump(\n",
    "        test_data,\n",
    "        config['test_out'],\n",
    "        compress=3)\n",
    "# end if\n",
    "\n",
    "#### save binarizer to output path\n",
    "if config['encoder_out']:\n",
    "    joblib.dump(\n",
    "        le,\n",
    "        config['encoder_out'],\n",
    "        compress=3)\n",
    "\n",
    "#### save encoder to output path\n",
    "if config['binarizer_out']:\n",
    "    joblib.dump(\n",
    "        lb,\n",
    "        config['binarizer_out'],\n",
    "        compress=3)\n",
    "# end def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10191"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
